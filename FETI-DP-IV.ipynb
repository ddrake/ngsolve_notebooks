{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FETI-DP in NGSolve IV: Inexact FETI-DP\n",
    "\n",
    "This time, we will, instead of going to the schur-complement for $\\lambda$,\n",
    "directly iterate on the equation\n",
    "$$\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "A_{\\scriptscriptstyle DP} & B^T \\\\\n",
    "B & 0\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "u\\\\\n",
    "\\lambda\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "f \\\\\n",
    "0\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The preconditioner will be \n",
    "$$\n",
    "\\widehat{M}^{-1} = \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "A_{\\scriptscriptstyle DP}^{-1} & 0 \\\\\n",
    "-M_s^{-1} B A_{\\scriptscriptstyle DP}^{-1} & M_s^{-1} \\\\\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "As $\\widehat{M}^{-1}$ is not symmetric, we will use GMRES.\n",
    "\n",
    "For setting up the preconditioner, we only need pieces we already have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_procs = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usrmeeting_jupyterstuff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cluster(num_procs)\n",
    "connect_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from ngsolve import *\n",
    "import netgen.meshing as ngmeshing\n",
    "from ngsolve.la import ParallelMatrix, FETI_Jump, SparseMatrixd, ParallelDofs, BlockMatrix\n",
    "from dd_toolbox import FindFEV, DPSpace_Inverse, ScaledMat, LinMat\n",
    "\n",
    "def load_mesh(nref=0):\n",
    "    ngmesh = ngmeshing.Mesh(dim=3)\n",
    "    ngmesh.Load('cube.vol')\n",
    "    for l in range(nref):\n",
    "        ngmesh.Refine()\n",
    "    return Mesh(ngmesh)\n",
    "\n",
    "def setup_space(mesh, order=1):\n",
    "    comm = MPI_Init()\n",
    "    fes = H1(mesh, order=order, dirichlet='right|top')\n",
    "    a = BilinearForm(fes)\n",
    "    u,v = fes.TnT()\n",
    "    a += SymbolicBFI(grad(u)*grad(v))\n",
    "    a.Assemble()\n",
    "    f = LinearForm(fes)\n",
    "    f += SymbolicLFI(x*y*v)\n",
    "    f.Assemble()\n",
    "    avg_dof = comm.Sum(fes.ndof) / comm.size\n",
    "    if comm.rank==0:\n",
    "        print('global,  ndof =', fes.ndofglobal, ', lodofs =', fes.lospace.ndofglobal)\n",
    "        print('avg DOFs per core: ', avg_dof)\n",
    "    return [fes, a, f]\n",
    "\n",
    "def setup_FETIDP(fes, a):\n",
    "    faces, edges, vertices = FindFEV(mesh.dim, mesh.nv, \\\n",
    "                                     fes.ParallelDofs(), fes.FreeDofs())\n",
    "    primal_dofs = BitArray([ v in set(vertices) for v in range(fes.ndof) ]) & fes.FreeDofs() \n",
    "    dp_pardofs = fes.ParallelDofs().SubSet(primal_dofs)\n",
    "    ar = [(num_e[0],d,1.0) for num_e in enumerate(edges) for d in num_e[1] ]\n",
    "    rows, cols, vals = [list(x) for x in zip(*ar)] if len(ar) else [[],[],[]]\n",
    "    B_p = SparseMatrixd.CreateFromCOO(rows, cols, vals, len(edges), fes.ndof)\n",
    "    edist_procs = [sorted(set.intersection(*[set(fes.ParallelDofs().Dof2Proc(v)) for v in edge])) for edge in edges]\n",
    "    eavg_pardofs = ParallelDofs(edist_procs, comm)\n",
    "    nprim = comm.Sum(sum([1 for k in range(fes.ndof) if primal_dofs[k] and comm.rank<fes.ParallelDofs().Dof2Proc(k)[0] ]))\n",
    "    if comm.rank==0:\n",
    "        print('# of global primal dofs: ', nprim)  \n",
    "    A_dp = ParallelMatrix(a.mat.local_mat, dp_pardofs)\n",
    "    dual_pardofs = fes.ParallelDofs().SubSet(BitArray(~primal_dofs & fes.FreeDofs()))\n",
    "    B = FETI_Jump(dual_pardofs, u_pardofs=dp_pardofs)\n",
    "    if comm.rank==0:\n",
    "        print('# of global multipliers = :', B.col_pardofs.ndofglobal)\n",
    "    A_dp_inv = DPSpace_Inverse(mat=a.mat, freedofs=fes.FreeDofs(), \\\n",
    "                               c_points=primal_dofs, \\\n",
    "                               c_mat=B_p, c_pardofs=eavg_pardofs, \\\n",
    "                               invtype_loc='sparsecholesky', \\\n",
    "                               invtype_glob='masterinverse')\n",
    "    F = B @ A_dp_inv @ B.T\n",
    "    innerdofs = BitArray([len(fes.ParallelDofs().Dof2Proc(k))==0 for k in range(fes.ndof)]) & fes.FreeDofs()\n",
    "    A = a.mat.local_mat\n",
    "    Aiinv = A.Inverse(innerdofs, inverse='sparsecholesky')\n",
    "    scaledA = ScaledMat(A, [0 if primal_dofs[k] else 1.0/(1+len(fes.ParallelDofs().Dof2Proc(k))) for k in range(fes.ndof)])\n",
    "    scaledBT = ScaledMat(B.T, [0 if primal_dofs[k] else 1.0/(1+len(fes.ParallelDofs().Dof2Proc(k))) for k in range(fes.ndof)])\n",
    "    Fhat = B @ scaledA @ (IdentityMatrix() - Aiinv @ A) @ scaledBT\n",
    "    nFhat = B @ scaledA @ (Aiinv @ A - IdentityMatrix()) @ scaledBT\n",
    "    return [A_dp, A_dp_inv, F, Fhat, nFhat, B, scaledA, scaledBT]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "comm = MPI_Init()\n",
    "mesh = load_mesh(nref=1)\n",
    "fes, a, f = setup_space(mesh, order=2)\n",
    "A_dp, A_dp_inv, F, Fhat, nFhat, B, scaledA, scaledBT = setup_FETIDP(fes, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the block-matrices\n",
    "\n",
    "For setting up the saddle point system and the preconditioner, \n",
    "we can use BlockMatrix.\n",
    "\n",
    "We could implement $\\widehat{M}^{-1}$ more efficiently as\n",
    "$$\n",
    "\\widehat{M}^{-1}\n",
    "=\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "I & 0 \\\\\n",
    "0 & M_s^{-1}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "I & 0 \\\\\n",
    "B & I\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "A_{\\scriptscriptstyle DP}^{-1} & 0 \\\\\n",
    "0 & I\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This would mean that we only have to apply $A_{\\scriptscriptstyle DP}^{-1}$ and $M_s^{-1}$ \n",
    "once instead of twice.\n",
    "\n",
    "However, this way we would still need multiple unnecessary vector copies.\n",
    "\n",
    "We can avoid both double applications and annecessary vector copies if we really quickly implement this ourselfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "M = BlockMatrix([[A_dp, B.T], \\\n",
    "                 [B, None]])\n",
    "Mhat = BlockMatrix([[A_dp_inv, None], \\\n",
    "                    [Fhat @ B @ A_dp_inv, nFhat]])\n",
    "class Mhat_v2(BaseMatrix):\n",
    "    def __init__(self, Ahat, B, Fhat):\n",
    "        super(Mhat_v2, self).__init__()\n",
    "        self.Ahat = Ahat\n",
    "        self.B = B\n",
    "        self.Fhat = Fhat\n",
    "        self.hv = Fhat.CreateColVector()\n",
    "    def Mult(self, x, y):\n",
    "        y[0].data = self.Ahat * x[0]\n",
    "        self.hv.data = x[1] - self.B * y[0]\n",
    "        y[1].data = self.Fhat * self.hv\n",
    "    def CreateRowVector(self):\n",
    "        return BlockVector([self.Ahat.CreateRowVector(), self.Fhat.CreateRowVector()])\n",
    "    def CreateColVector(self):\n",
    "        return BlockVector([self.Ahat.CreateRowVector(), self.Fhat.CreateRowVector()])\n",
    "Mhat2 = Mhat_v2(A_dp_inv, B, Fhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one small problem. \n",
    "We have to use the C++-side GMRES, simply because that is currently \n",
    "the only one available to us form NGSolve.\n",
    "\n",
    "BlockMatrix does not play nice with GMRES, because BlockMatrix works with\n",
    "BlockVectors, and GMRES expects standard NGSolve (Parallel-)Vectors.\n",
    "\n",
    "\"LinMat\" is a *temporary* simple workaround, that just copies between BlockVectors and\n",
    "normal \"linear\" vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "M_lin = LinMat(M, [A_dp.col_pardofs, B.col_pardofs])\n",
    "Mhat_lin = LinMat(Mhat, [A_dp.col_pardofs, B.col_pardofs])\n",
    "Mhat2_lin = LinMat(Mhat2, [A_dp.col_pardofs, B.col_pardofs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last annoyance is that jupyter-notebooks do not capture\n",
    "C++ stdout, so we lose the output during the iteration.\n",
    "\n",
    "**this only affects us inside the notebooks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "sol = M_lin.CreateRowVector()\n",
    "rhs = sol.CreateVector()\n",
    "rhs[:] = 0.0\n",
    "rhs[0:fes.ndof] = f.vec.local_vec\n",
    "rhs.SetParallelStatus(f.vec.GetParallelStatus())\n",
    "ngsglobals.msg_level = 3\n",
    "t1 = -comm.WTime()\n",
    "gmr = GMRESSolver(mat=M_lin, pre=Mhat_lin, maxsteps=100,\\\n",
    "                  precision=1e-6, printrates=True)\n",
    "sol.data = gmr * rhs\n",
    "t1 += comm.WTime()\n",
    "nsteps1 = gmr.GetSteps()\n",
    "t2 = -comm.WTime()\n",
    "gmr = GMRESSolver(mat=M_lin, pre=Mhat2_lin, maxsteps=100,\\\n",
    "                  precision=1e-6, printrates=True)\n",
    "sol.data = gmr * rhs\n",
    "t2 += comm.WTime()\n",
    "nsteps2 = gmr.GetSteps()\n",
    "if comm.rank==0:\n",
    "    print('\\ntook', nsteps1, 'steps for v1')\n",
    "    print('time solve v1: ', t1)\n",
    "    print('dofs per proc and second v1: ', fes.ndofglobal / ( t1 * comm.size))\n",
    "    print('\\ntook', nsteps2, 'steps for v2')\n",
    "    print('time solve v2: ', t2)\n",
    "    print('dofs per proc and second v2: ', fes.ndofglobal / ( t2 * comm.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 1\n",
    "for t in sorted(filter(lambda t:t['time']>0.5, Timers()), key=lambda t:t['time'], reverse=True):\n",
    "    print(t['name'], ':  ', t['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "t_chol = filter(lambda t: t['name'] == 'SparseCholesky<d,d,d>::MultAdd', Timers()).__next__()\n",
    "maxt = comm.Max(t_chol['time']) \n",
    "if t_chol['time'] == maxt:\n",
    "    print('timers from rank ', comm.rank, ':')\n",
    "    for t in sorted(filter(lambda t:t['time']>min(0.3*maxt, 0.5), Timers()), key=lambda t:t['time'], reverse=True):\n",
    "        print(t['name'], ':  ', t['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --targets 0:20\n",
    "maxt = max(Timers(), key=lambda t:t['time'])\n",
    "if maxt['time']>min(0.3*maxt['time'], 0.5):\n",
    "    print('max timer rank', comm.rank, ': ', maxt['name'], ' ', maxt['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
